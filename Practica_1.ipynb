{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "occupational-mechanism",
   "metadata": {},
   "source": [
    "Asignatura: Tipología y ciclo de vida de los datos.\n",
    "Alumnos: PABLO CHILLERÓN BEVIÁ y EVGENY MUZAREV GEVORGIAN\n",
    "\n",
    "Práctica 1.\n",
    "\n",
    "Esta practica consiste de 2 fases: Fase 1 Scraping y Fase 2 El Análisis. \n",
    "\n",
    "\n",
    "\n",
    "# FASE 1 SCRAPING.\n",
    "\n",
    "## Descripción.\n",
    "\n",
    "Con este script lo que se pretende es, recolectar las ofertas del portal bookings.com de las próximas dos semanas(14 días), a partir del momento de ejecución del script. Los criterios de las ofertas son: país España, actividad Playa, 1 persona adulta sin niños, para una noche. Es decir, el portal bookings.com nos muestra todas las ofertas de hoteles en España, que están situados en zonas costeras, cerca de la playa, para pernoctar una persona sin niños.\n",
    "\n",
    "La forma en la que se preve recoger y presentar los datos de las ofertas es un dataframe, que contendra las siguientes columnas(variables): ['name','stars','addr','country','city','postal_c','score','comments','beach','service_1','service_2',...'service_n', 'price']\n",
    "\n",
    "Las ofertas se presentan en forma de bloques, unos 25 bloques por página, que contienen el precio de la oferta y una información breve. Estos bloques contienen urls que llevan a las fichas de ofertas con la información más detallada.\n",
    "\n",
    "Este script recoge el nombre del hotel de la oferta (la variable 'name'), el nº de estrellas (la variable 'stars'), la dirección completa (la variable 'addr'), el país (la variable 'country'), la ciudad (la variable 'city'), el código postal (la variable 'postal_c'), el rating del hotel entre los usuarios (la variable 'score'), el nº de comentarios sobre hotel (la variable 'comments'), el rating sobre la playa (la variable 'beach'), los servicios que ofrece el hotel, como wifi gratis, parking, piscina, etc. (las variables llevarán el nombre de los servicios), estos servicios se presentarán en forma binaria: 1 el servicio presente en la oferta, 0 no. Finalmente, se recoge el precio de la oferta (la variable 'price'). \n",
    "\n",
    "Se prevé que el script recoja un dataframe de unos aproximadamente 15-16 variables, por unos 14000 registros (ofertas), ya que el portal bookings.com muestra unas mil ofertas por fecha.\n",
    "\n",
    "La lógica del script es la siguiente. Primero, se recogen las urls de todas las ofertas por fechas. Segundo, se extraen todas las variables de cada una de las urls recogidas en el primer paso.\n",
    "\n",
    "Para esto, se divide el proceso en dos. El primer proceso recolecta las urls de las ofertas, que son aproximadamente 14000 urls, y dura unos 30 minutos. El segundo proceso, que se encarga de la extracción de los datos, puede procesar unas 1000 páginas por hora, si se utiliza de manera secuencial, es decir una url detrás de otra, y unas 1000 urls cada 5 minutos, si se utiliza multi-threading. \n",
    "\n",
    "Si no se utiliza multi-threading, se tardaría más de 10 horas en completar el proceso. Utilizando multi-threading, se tarda 2-3 horas, aun así, el proceso completo es <strong>MUY LENTO</strong>.\n",
    "\n",
    "\n",
    "\n",
    "Para probar la funcionalidad del script, el nº de días está establecido a sólo 1 día, en lugar de 14. Mientras que el dataset utilizado para el análisis, contiene 14 días. Es decir, al ejecutar todo el código, se generará un dataset pequeño de aproximadamente 1000 entradas(depende de la cantidad de ofertas que haya el día que se ejecuta el código) después de completarse la fase de scraping(dataframe.csv), y para la fase del análisis, se utiliza el dataset completo de 14 días, que contiene 13943 registros(dataframe_13943.csv), generado con anterioridad. Todo esto hace que el código tarda unos 7-10 minutos como mucho en ejecutarse completamente. Con lo cual, se puede ejecutarlo sin miedo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-contractor",
   "metadata": {},
   "source": [
    "## 1. Extracción de las urls de las ofertas.\n",
    "\n",
    "\n",
    "La extracción consiste en crear un diccionario, que contendrá todas las urls de todas las ofertas, para cada una de las fechas, empezando desde la fecha en la cual se ejecuta el script + 13 días. El diccionario tendrá la fecha para la que se hace la búsqueda como la clave (key), y una lista de urls para esta fecha como valor(value).\n",
    "\n",
    "Para esto, creamos una lista de las urls de las páginas de respuesta a la búsqueda para cada una de las fechas. Estas páginas aparte de presentar los bloques con ofertas, también abajo del todo, presentan botones-enlaces a las siguientes páginas, que muestran otros 25 resultados(1,2,3,...n,..9). Las urls de estas páginas son distintas a las urls de las primeras páginas. Con lo cual, es necesario generar un segundo tipo de las urls a partir de la primera, modificándolos un poco y añadiendo la parte de búsqueda que siempre acaba en el número de resultados a mostrar (=25, =50, =75, etc). Abajo de la página, se muestran botones para pasar a la siguiente página (1,2,3....8,9). Sabiendo qué número es el último, sabemos cuantos pasos de 25 hay que dar. Por ejemplo, si el último número de los botones acaba en 7, sabemos que hay que incrementar 7 veces en 25 el último número de la url: =25 (se muestran resultados de 26 a 50), =50 (se muestran resultados de 51 a 75), =75(de 76-100),..., y así hasta 7 veces, 7x25=175. \n",
    "\n",
    "De esta manera, pasando por cada una de estas urls que contienen 25 ofertas, extraemos las urls de fichas de estas 25 ofertas. Es decir, primero construimos unas urls, para poder extraer otras.\n",
    "\n",
    "Estas últimas, se utilizarán ya para extraer los datos.\n",
    "\n",
    "Aparte de las urls, también se extraen los precios de las ofertas que se muestran en los bloques, ya que no ha sido posible extraerlos en el segundo proceso, por la complejidad de la implementación. Esto es debido a que los parsers que se utilizan en la librería bs4 (html_parser y lxml) son muy caprichosos y procesan html de las páginas de manera distinta. Al igual que pasa con los módulos de la librería requests, depende de cual se utiliza para establecer la conexión, influye, por lo visto, en cómo manda el código html el servidor. Todo esto presenta ciertas dificultades a la hora de implementar el parsing, ya que, primero el código html que el servidor manda al nuestro navegador y el que vemos en el 'Element Inspector' puede no coincidir con el código html que manda el servidor a petición del módulo de la librería 'requests'. Luego, cada parser (html_parser y lxml) procesa el código de manera distinta, hasta el punto que los tags que vemos en el 'Element Inspector', no están presentes en html que tenemos cargado en bs4. Realmente, ha sido bastante complicado ir jugando con los parsers y los módulos de 'request', para encontrar la combinación óptima.\n",
    "\n",
    "Por ejemplo, para extraer las urls de las fichas de ofertas y los nombres de hoteles, es mejor utilizar css selector, 'html_parser' y requests.get(), mientras que para extraer los datos de las ofertas, es mejor utilizar\n",
    "'lxml' parser y urllib.request.urlopen().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c566d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos librerías necesarias\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import lxml\n",
    "import pickle\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5\"\\\n",
    "           \"37.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"}\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-credits",
   "metadata": {},
   "source": [
    "### La función url_l_f(), que compone la urls de las páginas de resultados.\n",
    "\n",
    "Esta función es la que compone las urls de las páginas de los resultados de las ofertas, y que pasa estas urls a la siguiente función baige(), encargada de extraer las urls de las fichas de las ofertas y los precios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_l_f(url,pags):\n",
    "    \n",
    "    ##### Esta función recibe de 2 argumentos: la url inicial de los resultados    #####\n",
    "    ##### con las fichas de los hoteles, que servirá de base a partir de la cual   #####\n",
    "    ##### se van a construir las siguientes url'es consecutivas de la muestra de   #####\n",
    "    ##### los resultados(25,50,75,100,125,...,pags) y el número de páginas(pags)   #####\n",
    "    ##### de 25 resultados cada una, hasta que hay que contruir las URLs           ##### \n",
    "    ##### Finalmente, la función pasa cada una de las url'es generadas a la        #####\n",
    "    ##### baige(), que se encarga de extraer cada una de las url de las fichas de  #####\n",
    "    ##### hoteles, y sus precio y el nombre.                                       #####\n",
    "    \n",
    "    # Iniciamos el número de resultados a mostrar con un 0\n",
    "    b = 0\n",
    "    # Para cada una de las páginas\n",
    "    for i in range(0,pags):\n",
    "        # Incrementamos un paso(cada paso=página de 25 resultados)\n",
    "        b = b+25\n",
    "        # Añadimos el paso a la url base y la pasamos a la función baige()\n",
    "        baige(url+str(b),0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-state",
   "metadata": {},
   "source": [
    "### La función baige(), que extrae las urls de las fichas de ofertas y precios.\n",
    "\n",
    "Esta función recibe las urls procedentes de la función anterior url_l_f(), una a una, realiza una conexión 'request' al bookikgs.com y extrae las urls de las fichas de las ofertas y sus precios. Las urls, cada una con su identificador, se agregan a un diccionario directamente, igual que los precios y los nombres, cada uno con su identificador, se agregan en dos diccionarios separados. Los identificadores se utilizan para poder recrear el orden original de los registros, debido a que el orden se altera al utilizar multi-threading, y el orden original lo necesitamos para unir correctamente los precios al dataframe.\n",
    "\n",
    "El motivo por el cual se extrae el precio en esta parte del proceso y no junto con el resto de los datos, ya se ha expuesto mas arriba, y es que extraer el precio desde los bloques de ofertas de la página de resultados es más fácil que desde las fichas, que son casi imposibles de acceder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baige(url, x):\n",
    "    \n",
    "    ##### Esta función recibe una url de la página que contiene las fichas de      #####\n",
    "    ##### hoteles que salen en los resultados de la búsqueda, realiza una llamada  #####\n",
    "    ##### al servidor con la url pasada utilizando la librería requests, crea un   #####\n",
    "    ##### objeto 's' de bs4 con la sesión y el parser, scrapea las url'es de cada  #####\n",
    "    ##### una de las fichas de los hoteles que salen en la página y los agrega a   #####\n",
    "    ##### la lista ls[]. También devuelve las el objeto 's' con la sesión, y       #####\n",
    "    ##### también imprime por pantalla las urls que va agregando y el contador.    #####\n",
    "    \n",
    "    ##### Otra funcionalidad que ha sido añadida posteriormente, es recoger los    #####\n",
    "    ##### precios y nombres de los hoteles. \n",
    "    \n",
    "    # La llamada al servidor y el objeto 's'\n",
    "    r = requests.get(url, headers=headers)\n",
    "    s = BeautifulSoup(r.content, 'lxml')\n",
    "    \n",
    "    # El contador\n",
    "    global d\n",
    "    # Los identificadores\n",
    "    global id_u # de url\n",
    "    global id_p # de precio\n",
    "    global id_d # de fecha\n",
    "    \n",
    "    \n",
    "    # Accedemos a los elementos de las fichas que contienen partes de url y\n",
    "    # recreamos url completa\n",
    "    for f in s.select(\".f0b5ba03dc\"):\n",
    "        \n",
    "        # Imprimimos url por pantalla\n",
    "        print(f.a['href'].strip())\n",
    "        \n",
    "        # Imprimimos e incrementamos el contador\n",
    "        print(d)\n",
    "        d=d+1\n",
    "        \n",
    "        # Agregamos url al diccionario con su correspondiente id_u\n",
    "        dic_u[id_u]= f.a['href'].strip()\n",
    "        # Incrementamos identificador de url\n",
    "        id_u=id_u+1\n",
    "        # Agregamos la fecha al diccionario con su correspondiente id_d\n",
    "        dic_d[id_d] = date\n",
    "        # Incrementamos id_d \n",
    "        id_d=id_d+1\n",
    "    # Accedemos al precio, lo imprimimos por pantalla y lo agregamos \n",
    "    # al diccionario con su correspondiente id_p\n",
    "    for t in s.select('._e885fdc12'):\n",
    "        print(t.text.split('€')[1].strip())\n",
    "        dic_p[id_p] = t.text.split('€')[1].strip()\n",
    "        id_p=id_p+1\n",
    "    \n",
    "    # Por si acaso, controlamos sincronización\n",
    "    if not((id_u==id_p)&(id_p==id_d)):\n",
    "        # Si pasa algo, agregamos el nº de id en el cual ha pasado algo\n",
    "        alarm.append(id_u)\n",
    "        alarm.append(id_n)\n",
    "        alarm.append(id_p)\n",
    "        alarm.append(id_d)\n",
    "    # Devolvemos la sesión\n",
    "    if x==1:\n",
    "        return(s)\n",
    "    else:\n",
    "        print(\"ok\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-exhaust",
   "metadata": {},
   "source": [
    "### El script que arranca la primera parte del proceso, la extracción de urls.\n",
    "\n",
    "Primero se crea una lista de fechas, luego se componen las correspondientes urls para esta fechas y se van pasando, una a una a la función baige(), la cual descarga el contenido de la primera página de los resultados de la búsqueda, extrae la url y el precio del hotel, y los guarda en los diccionarios.\n",
    "\n",
    "También devuelve la sesión 's' de bs4 con la primera página cargada, para calcular el número de las páginas de resultados.\n",
    "\n",
    "A continuación, se llama a la función url_l_f(), pasándole las url de las siguientes páginas de resultados, a las cuales se les añade 25,50,75,100,..., hasta el número de las páginas, que se pasan como el segundo argumento. Esta función pasa las urls obtenidas a la función baige(), que extrae las urls de las fichas y los precios.\n",
    "\n",
    "Finalmente, se agregan las urls y precios extraídos a sus respectivos diccionarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca los próximos días como fechas\n",
    "prox_dias = 1 # Está puesto a 1 día, para que la ejecución no tarde demasiado, \n",
    "              # aunque para generar el dataset para el análisis, se ha utilizado los 14 días\n",
    "dates = [datetime.date.today() + timedelta(days=j) for j in range(0,prox_dias)]\n",
    "\n",
    "# Se crea una lista de url'es, por si acaso\n",
    "urls=[]\n",
    "# Se crea una especie de controlador para controlar la sincronización de\n",
    "# extracción de los datos\n",
    "alarm = []\n",
    "# Se crea un contador\n",
    "d=1\n",
    "# Se crean identificadores para la extracción de urls, precios y fechas\n",
    "id_u=0\n",
    "id_p=0\n",
    "id_d=0\n",
    "\n",
    "# Creamos el diccionario para las urls{id_u:url} \n",
    "dic_u = {}\n",
    "# Creamos diccionario de precios{id_p:precio}\n",
    "dic_p = {}\n",
    "# Creamos el diccionario para fechas{id_p:date}\n",
    "dic_d = {}\n",
    "\n",
    "# Para cada una de las fechas:\n",
    "for date in dates:\n",
    "    # Creamos una lista en la que se van a agregar los valores\n",
    "    ls = []\n",
    "    \n",
    "    # Extraemos componentes de las fechas de entrada/salida:      \n",
    "    checkin_date = date  \n",
    "    checkout_date = date + timedelta(days=1) \n",
    "    checkin_day = checkin_date.day\n",
    "    checkin_month = checkin_date.month\n",
    "    checkin_year = checkin_date.year\n",
    "    checkout_day = checkout_date.day\n",
    "    checkout_month = checkout_date.month\n",
    "    checkout_year = checkout_date.year\n",
    "    \n",
    "    # Recreamos url de la primera página de resultados de la búsqueda  \n",
    "    url_2 = 'https://www.booking.com/searchresults.es.html?aid=309654&label='\\\n",
    "    'booking-be-es-4hlnAGESyHVwkqBgv3aKIwS479808260559%3Apl%3Ata%3Ap1%3Ap22.563.000%3Aac%3Aap%3Aneg'\\\n",
    "    '%3Afi%3Atikwd-13273066%3Alp1005413%3Ali%3Adec%3Adm%3Appccp%3DUmFuZG9tSVYkc2RlIyh9YcsZ-Id2vkzIfTm'\\\n",
    "    'YhvC5HOg&sid=c4b3f6f420352b7e6268dbe166abff7d&tmpl=searchresults&ac_click_type=b&ac_position=0&'\\\n",
    "    'checkin_month='+str(checkin_date.month)+'&checkin_monthday='+str(checkin_date.day)+'&'\\\n",
    "    'checkin_year='+str(checkin_date.year)+'&checkout_month='+str(checkout_date.month)+'&'\\\n",
    "    'checkout_monthday='+str(checkout_date.day)+'&checkout_year='+str(checkout_date.year)+'&'\\\n",
    "    'city=-373226&class_interval=1&dest_id=197&dest_type=country&from_sf=1&g'\\\n",
    "    'roup_adults=1&group_children=0&label_click=undef&no_rooms=1&raw_dest_type=country&room1=A&'\\\n",
    "    'sb_price_type=total&search_selected=1&shw_aparth=1&slp_r_match=0&src=searchresults&srpvid='\\\n",
    "    '2cc557b3d2e20074&ss=Espa%C3%B1a&ss_raw=Esp&ssb=empty&ssne=Benidorm&ssne_untouched=Benidorm&top_ufis=1&'\\\n",
    "    'sig=v1RzUHRY77&nflt=popular_activities%3D302%3Bht_id%3D204%3B&percent_htype_hotel=1&rsf='\n",
    "    \n",
    "    # Recreamos el segundo tipo de url de las páginas de resultados(las que muestran \n",
    "    #1-25, 26-50,51-75,...,etc), primera parte\n",
    "    a = 'https://www.booking.com/searchresults.es.html?aid=309654&label='\\\n",
    "    'booking-be-es-4hlnAGESyHVwkqBgv3aKIwS479808260559%3Apl%3Ata%3Ap1%3Ap22.563.000%3Aac%3Aap%3Aneg'\\\n",
    "    '%3Afi%3Atikwd-13273066%3Alp1005413%3Ali%3Adec%3Adm%3Appccp%3DUmFuZG9tSVYkc2RlIyh9YcsZ-Id2vkzIfTm'\\\n",
    "    'YhvC5HOg&sid=c4b3f6f420352b7e6268dbe166abff7d&tmpl=searchresults&ac_click_type=b&ac_position=0&'\\\n",
    "    'checkin_month='+str(checkin_date.month)+'&checkin_monthday='+str(checkin_date.day)+'&'\\\n",
    "    'checkin_year='+str(checkin_date.year)+'&checkout_month='+str(checkout_date.month)+'&'\\\n",
    "    'checkout_monthday='+str(checkout_date.day)+'&'\\\n",
    "    'checkout_year='+str(checkout_date.year)+'&city=-373226&class_interval=1&dest_id=197&dest_type=country&'\n",
    "    \n",
    "    # Segunda parte\n",
    "    b = 'dtdisc=0&from_sf=1&group_adults=1&group_children=0&inac=0&index_postcard=0&label_click=undef&'\\\n",
    "    'nflt=popular_activities%3D302%3Bht_id%3D204%3B&no_rooms=1&percent_htype_hotel=1&postcard=0&'\\\n",
    "    'raw_dest_type=country&room1=A&sb_price_type=total&search_selected=1&shw_aparth=1&slp_r_match=0&'\\\n",
    "    'src=searchresults&srpvid=65e56cb746a10078&ss=Espa%C3%B1a&ss_all=0&ss_raw=Esp&ssb=empty&sshis=0&'\\\n",
    "    'ssne=Benidorm&ssne_untouched=Benidorm&top_ufis=1&sig=v1hAlQhinU&rows=25&offset='\n",
    "    \n",
    "    # Juntamos las dos partes y tenemos una urla inicial, a la cual vamos a ir\n",
    "    # añadiendo los resultados(25,50,75,100,...) que se mostrarán en la página.\n",
    "    # 25 resultados por página.\n",
    "    p_url = a+b\n",
    "    \n",
    "    #urls.append(url_2)\n",
    "    \n",
    "    # Añadimos el primer valor a la lista, que será la fecha\n",
    "    ls.append(date)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Llamamos la función baige() que extrae url de las fichas de los hoteles que se muestran en\n",
    "    # la página de resultados y los guarda en una lista. La función también devuelve s(un objeto \n",
    "    # de bs4 con la primera página de resultados de la búsqueda para cada fecha)\n",
    "    s=baige(url_2,1)\n",
    "    \n",
    "    ### Cálculo de nº de páginas de resultados: Nºres.devueltos/25 = nºde páginas ###\n",
    "    \n",
    "    # Accedemos al elemento de la página, que muestra el número de resultados de la búsqueda y \n",
    "    # si devuelve más de 1000 resultados, por ejemplo 1346, lo dejamos a 1000, ya que el servidor\n",
    "    # de booking no muestra más de 1000 resultados, que son 40 páginas de 25 resultados en cada una\n",
    "    if '.' in s.find('div',{'id':'bodyconstraint'}).div.find('div',{'id':'searchresultsTmpl'}).div.div.h1.text.split(' ')[1].strip():\n",
    "        pags = 39\n",
    "    # Si la búsqueda devuelve menos de 1000 resultados, se deviden entre 25, se redondea\n",
    "    # al número mayor siguiente, si la división devuelve decimales\n",
    "    else:\n",
    "        v = int(s.find('div',{'id':'bodyconstraint'}).div.find('div',{'id':'searchresultsTmpl'}).div.div.h1.text.split(' ')[1].strip())\n",
    "        pags = int(v/25) + (v%25 > 0)\n",
    "    \n",
    "    \n",
    "    # Se llama la función url_l_f(), a la que se le pasa la p_url, url que se ha preparado\n",
    "    # anteriormente, que es la url inicial de todas las páginas de resultados, que acaba en\n",
    "    # \"https://www.booking.com/../..offset=\", y a la que se le va añadiendo offset=25,\n",
    "    # offset=50, offset=75, offset=100, etcétera, en función del número de páginas(pags).\n",
    "    # Luego, se guarda la url de cada una de las fichas de hoteles que se muestran en\n",
    "    # la página, unas 25, y se van añadiendo a la lista ls[].\n",
    "    url_l_f(p_url,pags)\n",
    "    \n",
    "    # Finalmente, guardamos la lista ls[] en el diccionario que hemos creado anteriormente.\n",
    "    # Para cada fecha(que son 14), guardaremos la lista ls[], con sus correspondientes \n",
    "    # url'es, que hemos recogido con las funciones baige()(para recoger url's de primera\n",
    "    # página) y url_l_f(), para recoger url'es del resto de la página, para cada una de\n",
    "    # las 14 fechas. \n",
    "    #dic[date]=ls\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos si en la lista alarm hay algún valor\n",
    "if bool(len(alarm)):\n",
    "    print(alarm)\n",
    "else:\n",
    "    print(id_u)\n",
    "    print(id_p)\n",
    "    print(id_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Guardamos el diccionario con las urls\n",
    "a_file = open(\"dic_u.pkl\", \"wb\")\n",
    "\n",
    "pickle.dump(dic_u, a_file)\n",
    "\n",
    "a_file.close()\n",
    "\n",
    "# Guardamos el diccionaro de precios\n",
    "a_file = open(\"dic_p.pkl\", \"wb\")\n",
    "\n",
    "pickle.dump(dic_p, a_file)\n",
    "\n",
    "a_file.close()\n",
    "\n",
    "\n",
    "# Guardamos el diccionaro de fechas\n",
    "a_file = open(\"dic_d.pkl\", \"wb\")\n",
    "\n",
    "pickle.dump(dic_d, a_file)\n",
    "\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-painting",
   "metadata": {},
   "source": [
    "## 2. Extracción de los datos de las ofertas.\n",
    "\n",
    "Esta parte de proceso se encarga de escrapear cada una de las urls obtenidas en la fase anterior. Al final de esta parte, tendremos una lista enorme con los valores de las variables, que luego se pasará al formato del dataframe, y un diccionario con los servicios que ofrece cada hotel y que se unirá posteriormente al dataframe resultante en forma binaria: 1 si el servicio está presente en la oferta, 0 si no.\n",
    "\n",
    "Al implementar multi-threading, se altera el orden en el cual se agregan los valores a la lista de las urls pasadas. Es decir, el orden de los valores en la lista devuelta no es el mismo en el que se pasaban las urls. Pero en nuestro caso el orden importa, ya que todavía tenemos que unir los precios al dataframe, que están en el mismo orden en el que se han extraído. Con lo cual, es necesario inventar un identificador que indique el orden en el cual se pasan las urls de entrada, para poder luego reconstruir el orden original de la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la lista resultante que posteriormente se pasará al\n",
    "# dataframe\n",
    "res=[]\n",
    "# Creamos el diccionario con nombres de hoteles y servvicios\n",
    "# que ofrecen\n",
    "dic={}\n",
    "# Creamos contador\n",
    "d_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-dynamics",
   "metadata": {},
   "source": [
    "### La función dop(), que se encarga de extraer los datos.\n",
    "\n",
    "Esta función recibe una url, fecha y un identificador coomo argumentos, realiza una conexión 'request' para la url de la ficha de oferta pasada y va extrayendo los datos, añadiéndolos a una lista temporal, como si fuera una fila del dataframe, que posteriormente se incorporará a la lista resultante res[]. Esta fila tendrá la siguiente estructura: ['id','date','name','stars','addr','country','city','postal_c','score','comments','beach'].\n",
    "\n",
    "El diccionario de servicios tendrá i_d como la clave y una lista de servicios en forma ['nombre','servicio_1', servicio_2, servicio_3,..., servicio_n] como valor. Posteriormente se unirán los servicios que presta cada hotel en su oferta, al dataframe resultante en forma binaria: 1 si está presente, 0 si no.\n",
    "\n",
    "El identificador i_d se utilizará para reconstruir el orden original de las urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dop(ur, date, i_d):\n",
    "    \n",
    "    ''' Esta función recibe 3 argumentos, una url, una \n",
    "        fecha(date) y un identificador(i_d). No devuelve nada.\n",
    "        Realiza una conexión request con el servido para la\n",
    "        url pasada de argumento, extrae los datos de ofertas\n",
    "        de las páginas y los agrega a la lista res[] y al\n",
    "        diccionario dic[]'''\n",
    "    \n",
    "    # Declaramos la variable del contador como global\n",
    "    global d_count\n",
    "    \n",
    "    # Creamos una lista temporal que dura una iteración\n",
    "    lst = []\n",
    "    # Agregamos identificador a la lista\n",
    "    lst.append(i_d)\n",
    "    # Agregamos la fecha a la lsita, que será el valor de la primera\n",
    "    # columna \"Date\" del dataframe\n",
    "    lst.append(date)\n",
    "    # Imprimimos la fecha por pantalla\n",
    "    print(date)\n",
    "    # Creamos sesión y objeto de bs4 con la url\n",
    "    r = urllib.request.urlopen(ur)\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "        \n",
    "    # Utilizamos la construcción try-except para que no se nos\n",
    "    # detenga la ejecución en el caso de errores\n",
    "    try:\n",
    "       # Accedemos al nombre del hotel\n",
    "        nombre = soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "            find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "            find('div', {'id':'wrap-hotelpage-top'}).\\\n",
    "            find('div',{'class':'hp__hotel-title'}).h2.text.split('\\n')[2].strip()\n",
    "        # Lo imprimimos por pantalla\n",
    "        print('Nombre: '+str(nombre))\n",
    "        # Agregamos el nombre a la lista, que será el valor de la segunda\n",
    "        # columna del dataframe \n",
    "        lst.append(nombre)\n",
    "            \n",
    "    # En el caso del error, imprimimos un mensaje por pantalla\n",
    "    # y simplemente agregamos un nan con el número de iteración a la lista\n",
    "    except:\n",
    "        print('nombre nan')\n",
    "        nombre = str('nan_'+ str(d_count))\n",
    "        lst.append(nombre)\n",
    "        \n",
    "    # Extraemos el número de estrellas\n",
    "    try: \n",
    "        # Accedemos al elemnto que muestra las estrellitas y contamos\n",
    "        # el número de <span>(que es cada estellita) que contiene\n",
    "        stars = len(soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                             find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                             find('div', {'id':'wrap-hotelpage-top'}).\\\n",
    "                             find('div',{'class':'hp__hotel-title'}).\\\n",
    "                             find('span',{'class':'hp__hotel_ratings'}).span.div.div.\\\n",
    "                             span.div.span.find_all('span'))\n",
    "            \n",
    "        # Mostramos el número de estrellas por pantalla\n",
    "        print('Estrellas: '+str(stars))\n",
    "        # Agregamos el número de estrellas a la lista, que será el\n",
    "        # valor de la \"Stars\" del dataframe\n",
    "        lst.append(stars)\n",
    "            \n",
    "    # En el caso del error, imprimimos un mensaje por pantalla y\n",
    "    # agregamos un nan a la lista\n",
    "    except:\n",
    "        print('estrellas_nan')\n",
    "        stars = 'nan'\n",
    "        lst.append(stars)\n",
    "        \n",
    "    # Extraemos la dirección\n",
    "    try:\n",
    "        # Accedemos al elemento que muetsra la drección y la \n",
    "        # extraemos\n",
    "        addr = soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                          find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                          find('div', {'id':'wrap-hotelpage-top'}).p.text.split('\\n')[5].strip()\n",
    "            \n",
    "        # Imprimimos por pantalla\n",
    "        print('Dirección: '+str(addr))\n",
    "        # Agregamos la dirección a la lista, que será el valor de la\n",
    "        # columna \"Address\" del dataframe, la dirección completa\n",
    "        lst.append(addr)\n",
    "            \n",
    "    # En el caso de error, imprimimos un mensaje y agregamos un nan \n",
    "    # a la lista\n",
    "    except:\n",
    "        print('addr nan')\n",
    "        addr = 'nan'\n",
    "        lst.append(addr)\n",
    "            \n",
    "    # También extraemos el país, la ciudad y el código postal\n",
    "    # por separado.\n",
    "        \n",
    "    # Accedemos y extraemos el país\n",
    "    try:    \n",
    "        country = soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                           find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                           find('div', {'id':'wrap-hotelpage-top'}).p.text.split('\\n')[5].\\\n",
    "                           strip().split(', ')[-1].strip()\n",
    "        # Imprimimos por pantalla\n",
    "        print('País: '+str(country))\n",
    "        # Agregamos el país a la lista, que será el valor de la columna\n",
    "        # \"Country\" del dataframe resultante\n",
    "        lst.append(country)\n",
    "        \n",
    "    # En el caso del error, imprimimos un mensaje y agregamos \n",
    "    # un nan a la lista\n",
    "    except:\n",
    "        print('country nan')\n",
    "        country = 'nan'\n",
    "        lst.append(country)\n",
    "            \n",
    "    # Accedemos y extraemos a la ciudad\n",
    "    try:\n",
    "        city = soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                    find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                        find('div', {'id':'wrap-hotelpage-top'}).p.text.split('\\n')[5].\\\n",
    "                        strip().split(', ')[-2].split(' ')[-1].strip()\n",
    "            \n",
    "        # Imprimimos por pantalla\n",
    "        print('Ciudad: '+str(city))\n",
    "        # Agregamos la ciudad a la lista, que será el valor de la columna \n",
    "        # \"City\" del datframe\n",
    "        lst.append(city)\n",
    "           \n",
    "    # En el caso del error, avisamos y agregamos un nan a la lista\n",
    "    except:\n",
    "        print('city nan')\n",
    "        city = 'nan'\n",
    "        lst.append(city)   \n",
    "        \n",
    "    # Accedemos y estraemos el código postal\n",
    "    try:\n",
    "        postal_c = soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                            find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                            find('div', {'id':'wrap-hotelpage-top'}).p.text.split('\\n')[5].\\\n",
    "                            strip().split(', ')[-2].split(' ')[0].strip()\n",
    "            \n",
    "        # Imprimimos por pantalla\n",
    "        print('Cód. postal: '+str(postal_c))\n",
    "        # Agregamos el código a la lista, que será el valor de la columna\n",
    "        # \"Postal_code\" del dataframe\n",
    "        lst.append(postal_c)\n",
    "            \n",
    "    # En el caso del error, avisamos y agregamos un nan a la lista\n",
    "    except:\n",
    "        print('postal_c nan')\n",
    "        postal_c = 'nan'\n",
    "        lst.append(postal_c)\n",
    "            \n",
    "    # Accedemos y extraemos el valor de score\n",
    "    try:\n",
    "        score = soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                          find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                          find('div',{'id':'hotelTmpl'}).\\\n",
    "                          find('div',{'id':'basiclayout'}).\\\n",
    "                          find('div',{'id':'right'}).\\\n",
    "                          find('div',{'id':'blockdisplay1'}).div.div.div.\\\n",
    "                          find('div',{'class':'hp-gallery-review'}).div.div.a.div.div.div.div.div.text.strip()\n",
    "        \n",
    "        # Lo mostramos por pantalla\n",
    "        print('Score: '+str(score))\n",
    "        # Agregamos el valore de score a la lista, que será el valor\n",
    "        # de la columna \"Score\" del dataframe\n",
    "        lst.append(score)\n",
    "            \n",
    "    # En el caso del error, avisamos y agregamos un nan a la lista \n",
    "    except:\n",
    "        print('score nan')\n",
    "        score = 'nan'\n",
    "        lst.append(score)\n",
    "        \n",
    "    # Accedemos y extraemos los comentarios\n",
    "    try:\n",
    "        # La estructura del elemento que contiene el número de los comentarios\n",
    "        # puede variar, con lo cual se preven dos tipos. Aunque, debido a la\n",
    "        # cantidad de páginas, es deficil prever todas las estructuras.\n",
    "        # Con lo cual, si no es un tipo, se accede al otro.\n",
    "        if soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                          find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                          find('div',{'id':'hotelTmpl'}).\\\n",
    "                          find('div',{'id':'basiclayout'}).\\\n",
    "                          find('div',{'id':'right'}).\\\n",
    "                          find('div',{'class':'hp_nav_bar_wrapper hp-nav-bottom-border bui-spacer--large'}):\n",
    "            comments = soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                          find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                          find('div',{'id':'hotelTmpl'}).\\\n",
    "                          find('div',{'id':'basiclayout'}).\\\n",
    "                          find('div',{'id':'right'}).\\\n",
    "                          find('div',{'class':'hp_nav_bar_wrapper hp-nav-bottom-border bui-spacer--large'}).ul\\\n",
    "                          .span.text.split('(')[1].split(')')[0].strip()\n",
    "        else:\n",
    "            comments = soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                          find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                          find('div',{'id':'hotelTmpl'}).\\\n",
    "                          find('div',{'id':'basiclayout'}).\\\n",
    "                          find('div',{'id':'right'}).\\\n",
    "                          find('div',{'class':'hp_nav_bar_wrapper hp-nav-bottom-border'}).ul\\\n",
    "                          .span.text.split('(')[1].split(')')[0].strip()\n",
    "\n",
    "\n",
    "            \n",
    "        # Imprimimos por pantalla el número de los comentarios\n",
    "        print('Comments: '+str(comments))\n",
    "        # Se agregan los comentarios a la lista, que será el valor\n",
    "        # de la variable \"Comments\" del dataframe\n",
    "        lst.append(comments)\n",
    "        \n",
    "    # En el caso del error, avisamos y agregamos un nan a la lista\n",
    "    except:\n",
    "        print('comments nan')\n",
    "        comments = 'nan'\n",
    "        lst.append(comments)\n",
    "\n",
    "    # Accedemos y extraemos el valor de beach. Igual que con\n",
    "    # la variable anterior 'comments', se prueban 2 tipos de\n",
    "    # estructura del elemnto contiene el valor de beach\n",
    "    try:\n",
    "            \n",
    "        if soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                    find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                    find('div',{'id':'hotelTmpl'}).\\\n",
    "                    find('div',{'id':'basiclayout'}).\\\n",
    "                    find('div',{'id':'right'}).\\\n",
    "                    find('div',{'id':'blockdisplay1'}).div.\\\n",
    "                    find('div',{'class':'clearfix bh-photo-grid bh-photo-grid--space-down fix-score-hover-opacity'}):\n",
    " \n",
    "\n",
    "                \n",
    "            beach = soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                             find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                             find('div',{'id':'hotelTmpl'}).\\\n",
    "                             find('div',{'id':'basiclayout'}).\\\n",
    "                             find('div',{'id':'right'}).\\\n",
    "                             find('div',{'id':'blockdisplay1'}).div.\\\n",
    "                             find('div',{'class':'clearfix bh-photo-grid bh-photo-grid--space-down fix-score-hover-opacity'}).\\\n",
    "                             find('div',{'class':'hp-gallery-review'}).div.\\\n",
    "                             find('div',{'class':'best-review-score hp_lightbox_score_block'}).span.text.split('\\n')[2].strip()\n",
    "                \n",
    "        else:\n",
    "            beach = soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                             find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                             find('div',{'id':'hotelTmpl'}).div.div.\\\n",
    "                             find('div',{'id':'wrap-hotelpage-top'}).\\\n",
    "                             find('div',{'class':'hp__hotel-title'}).\\\n",
    "                             find_all('span')[12].text.split('\\n')[3].strip()\n",
    "        \n",
    "        # Imprimimos por pantalla\n",
    "        print('Playa: '+str(beach))\n",
    "        # Agregamos el valor de 'beach' a la lista, que será el\n",
    "        # valor de la columna \"Beach\" del dataframe\n",
    "        lst.append(beach)\n",
    "            \n",
    "    # En el caso del error, avisamos y agregamos un nan a la lista\n",
    "    except:\n",
    "        print('beach nan')\n",
    "        beach = 'nan'\n",
    "        lst.append(beach)\n",
    "\n",
    "\n",
    "    # Accedemos al elemento de la página que contiene los servicios.\n",
    "    # Debido a que es un conjunto de servicios, hacemos un diccionario\n",
    "    # con el nombre del hotel como clave, y la lista de servicios\n",
    "    # como valor. Luego, agregamos los servicios al dataframe en forma\n",
    "    # binaria, de manera que cada servicio será una columna del dataframe\n",
    "    # y contendrá un 1 si el servicio está presente en la fila(oferta), y\n",
    "    # un 0 si no. \n",
    "    try:\n",
    "        # Accedemos al elemento de la página que contiene el conjunto\n",
    "        # de servicios\n",
    "        services = soup.find('div',{'id':'bodyconstraint'}).\\\n",
    "                          find('div',{'id':'bodyconstraint-inner'}).\\\n",
    "                          find('div',{'id':'hotelTmpl'}).\\\n",
    "                          find('div',{'id':'basiclayout'}).\\\n",
    "                          find('div',{'id':'right'}).\\\n",
    "                          find('div',{'id':'blockdisplay1'}).div.\\\n",
    "                          find('div',{'class':'hp_hotel_description_hightlights_wrapper'}).\\\n",
    "                          find('div',{'class':'hotel_description_wrapper_exp hp-description'}).\\\n",
    "                          find('div',{'class':'hp_desc_important_facilities clearfix hp_desc_important_facilities--bui'}).\\\n",
    "                          find_all('div')\n",
    "        # Creamos una lista\n",
    "        ls_serv = [] \n",
    "           \n",
    "        # Agregamos la fecha\n",
    "        ls_serv.append(date)\n",
    "        \n",
    "        # Agregamos el nombre\n",
    "        ls_serv.append(nombre)\n",
    "        \n",
    "        # Iteramos por el conjunto de servicios\n",
    "        for service in services:\n",
    "            # Extraemos el nombre de servicio y lo agregamos a la\n",
    "            # lista\n",
    "            #print(service['data-name-en'].strip())\n",
    "            ls_serv.append(service['data-name-en'].strip())\n",
    "        # Agregamos el nombre del hotel con sus correspondientes\n",
    "        # servicios al diccionario\n",
    "        dic[i_d]=ls_serv\n",
    "                     \n",
    "    # En el caso de no poder extraer los servicios, avisamos y\n",
    "    # agregamos un nan a la lista\n",
    "    except:\n",
    "        print('services nan')\n",
    "        dic[i_d]='nan'\n",
    "        \n",
    "    # Finalmente, obtenemos una lista, que será una fila del dataframe con \n",
    "    # la siguiente estructura: ['name','stars','addr','country','city','postal_c','score','comments',beach]\n",
    "    # Agregamos esta lista(fila) a la lista resultante, que se transformará en \n",
    "    # el dataframe.\n",
    "    res.append(lst)\n",
    "        \n",
    "    # Imprimimos el nº del registro\n",
    "    print()\n",
    "    print('Registro nº: '+str(d_count))\n",
    "    print()\n",
    "    print()\n",
    "    # Incrementamos el contador\n",
    "    d_count = d_count+1\n",
    "    lock.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-departure",
   "metadata": {},
   "source": [
    "Arrancamos el multithreading con 20 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "lock = threading.Semaphore(20)\n",
    "def dop_mt(url_lst):\n",
    "    \n",
    "    global i_d\n",
    "    thread_pool = []\n",
    "\n",
    "    for i,url in enumerate(url_lst):\n",
    "        \n",
    "        thread = threading.Thread(target=dop, args=(url,dic_d[i],i,))\n",
    "        thread_pool.append(thread)\n",
    "        thread.start()\n",
    "        \n",
    "\n",
    "        \n",
    "        lock.acquire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "dop_mt(list(dic_u.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dejamos unos 30 segundos para que acaben todos los threads\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-command",
   "metadata": {},
   "source": [
    "Guardamos la lista res[] y el diccionario dic_serv[]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"res.pkl\", \"wb\")\n",
    "\n",
    "pickle.dump(res, a_file)\n",
    "\n",
    "a_file.close()\n",
    "\n",
    "\n",
    "\n",
    "a_file = open(\"dic_serv.pkl\", \"wb\")\n",
    "\n",
    "pickle.dump(dic, a_file)\n",
    "\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-trigger",
   "metadata": {},
   "source": [
    "## 3. Pasamos los datos al dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la lista y el diccionario\n",
    "\n",
    "a_file = open(\"res.pkl\", \"rb\")\n",
    "\n",
    "res_2 = pickle.load(a_file)\n",
    "\n",
    "a_file.close()\n",
    "\n",
    "\n",
    "a_file = open(\"dic_serv.pkl\", \"rb\")\n",
    "\n",
    "dic_serv = pickle.load(a_file)\n",
    "\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res, columns=['Id','Date','Name','Stars','Address','Country','City',\\\n",
    "                                'Postal_Code','Score','Comments','Beach'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-diary",
   "metadata": {},
   "source": [
    "Agregamos las columnas de servicios al dataframe.\n",
    "\n",
    "Primero, necesitamos extraer los valores únicos de servicios desde el diccionario, para dar nombre a columnas.\n",
    "Para esto, fundimos las listas de servicios que contiene el diccionario en una única lista.\n",
    "\n",
    "Luego, pasamos esta lista por set(), que nos devuelve los valores únicos.\n",
    "\n",
    "Y por último, simplemente añadimos columnas al dataframe con los nombres de estos valores únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la lista\n",
    "ls = []\n",
    "\n",
    "# Fundimos las listas en una única, dejando fuera los elementos 0 y 1, \n",
    "# ya que son fecha y el nombre, y nosotros sólo necesitamos servicios\n",
    "for keys, values in dic.items():\n",
    "    ls.extend(dic[keys][2:])\n",
    "    \n",
    "# Pasamos la lista por el set y ya tenemos los valores únicos\n",
    "serv_l = list(set(ls))\n",
    "\n",
    "# Se ha colado ahí un valor raro y lo eliminamos\n",
    "#serv_l.remove('n')\n",
    "\n",
    "# Creamos las columnas de servicios a partir de la lista de\n",
    "# valores únicos y las dejamos a 0\n",
    "for serv in serv_l:\n",
    "    df[serv] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-cooperative",
   "metadata": {},
   "source": [
    "Ahora, tenemos que cuadrar el diccionario con los servicios que ofrecen los hoteles con el dataframe. Para esto, hemos marcado cada fila del dataframe y cada lista del diccionario con un identificador, que en teoría deberían de corresponder.\n",
    "\n",
    "Usamos una doble condición para estar más seguros, queremos que coincidan id's y que el nombre del hotel del dataframe corresponda con el nombre que está en la lista de servicios, y una vez identificadas las dos cosas, se procede a buscar, servicio por servicio, si el servicio está en la lista o no. Si está, se agrega un 1 a la columna del correspondiente servicio y fila, si no está - un 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteramos por las filas del dataframe\n",
    "for index, row in df.iterrows():\n",
    "    # Para la fila de turno, iniciamos iteraciones por el diccionario\n",
    "    for key, value in dic.items():\n",
    "        # Comprobamos si coincide el valor de la columna Id del dataframe\n",
    "        # con la clave del diccionario, que debería de llevar el mismo id\n",
    "        if (row.Id==key)&(row.Name==dic[key][1]):\n",
    "            # En el caso positivo, se procede a buscar, servicio por servicio,\n",
    "            # si está presente en la lista o no\n",
    "            for col in df.columns[11:]:\n",
    "                if col in dic[key][2:]:\n",
    "                    # Si está presente, se agrega un 1 a la celda del\n",
    "                    # correspondiente sevicio\n",
    "                    df.iloc[index, df.columns.get_loc(col)]=1\n",
    "                else:\n",
    "                    #df.iloc[index][col] = 0\n",
    "                    # Se agrega un 0 en el caso negativo\n",
    "                    df.iloc[index, df.columns.get_loc(col)]=0\n",
    "        else:\n",
    "            next\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-skating",
   "metadata": {},
   "source": [
    "Por último, vamos a unir los precios al dataframe. \n",
    "Se supone que el diccionario de precios y las urls antes de pasarlas al multi-threading estaban en el mismo orden. Con lo cual, se pasó como argumento un identificador i_d con cada url. Ahora, podemos reconstruir el orden original, utilizando sort por la columna 'Id'. Una vez ordenado el dataframe, simplemente procedemos a unir los precios con el dataframe y ya está.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el diccionario\n",
    "a_file = open(\"dic_p.pkl\", \"rb\")\n",
    "\n",
    "dic_p = pickle.load(a_file)\n",
    "\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenamos el dataframe por la columna 'Id'\n",
    "df = df.sort_values('Id')\n",
    "\n",
    "# Unimos los precios\n",
    "df['Price'] = list(dic_p.values())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el dataframe de 1 día\n",
    "df.to_csv('dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataframe de 14 días\n",
    "df = pd.read_csv(\"dataframe_13943.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-maximum",
   "metadata": {},
   "source": [
    "Repasamos un poco el dataset, vemos qué es lo que podemos hacer con los nan's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Echamos un vistazo a las columnas que tenemos\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función que nos muestre los NaNs de cada columna\n",
    "def nans(df):\n",
    "    for col in df.columns:\n",
    "        if (df[col].isna().any()):\n",
    "            print(\"The column '\"+col+\"' has {} NaNs\".format(df[col].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "nans(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empezamos, por ejemplo, con la columna 'City', que tiene sólamente 2 valores NaN\n",
    "df[df.City.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-numbers",
   "metadata": {},
   "source": [
    "Podemos ver que las cuatro columnas que tienen 2 NaNs cada una('Address', 'Country', 'City' y 'Postal_Code'), están en estas dos filas y pertenecen a un mismo hotel 'Vincci Selección Estrella del Mar'. Lo que podemos hacer es mirar qué valores tienen estas columnas en otros registros de este hotel y sustituir manualmente los NaNs por estos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Name=='Vincci Selección Estrella del Mar'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sustituimos los NaNs\n",
    "df.iloc[13301,[4,5,6,7]] = ['KM 190,5 A7 (Antigua Carretera N340)'\\\n",
    "                            ' Avenida José de Ribera nº 25 ( Urbanización Real de Zaragoza),'\\\n",
    "                            ' 29604 Marbella, España','España','Marbella', 29604]\n",
    "\n",
    "df.iloc[12880,[4,5,6,7]] = ['KM 190,5 A7 (Antigua Carretera N340)'\\\n",
    "                            ' Avenida José de Ribera nº 25 ( Urbanización Real de Zaragoza),'\\\n",
    "                            ' 29604 Marbella, España','España','Marbella', 29604]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "nans(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-parts",
   "metadata": {},
   "source": [
    "Después de arreglar los valores NaNs del hotel 'Vincci Selección Estrella del Mar', ya sólo nos quedan 4 columnas con los NaN's. Vamos a seguir con la columna 'Comments', que tiene 5 NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Comments.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-lottery",
   "metadata": {},
   "source": [
    "Después de revisar la página del hotel 'Hôte The Corralejo Beach' en booking.com, resulta que tanto los valores 'Score' como 'Comments' están ausentes. Con lo cual, al haber nada más que 5 registros, podemos prescindir de ellos directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los registros del hotel 'Hôte The Corralejo Beach'\n",
    "df = df.drop(df.index[[8860,12916,12922,13933,8911]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseteamos los índices\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a ver los NaNs\n",
    "nans(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-hunter",
   "metadata": {},
   "source": [
    "Ya sólo nos quedan tres columnas que contienen los valores NaN.\n",
    "\n",
    "Vamos a seguir con la variable 'Score', que sólo tiene 11 NaNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Score.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-black",
   "metadata": {},
   "source": [
    "Vemos que los hoteles 'Añoreta Suites', 'Casa Leon' y 'Arena Surf Hotel' no tienen los valores 'Score' ni tampoco 'Stars', en sus correspondientes páginas en booking.com tampoco los tienen. Con lo cual, también vamos a eliminar estos 11 registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los registros de los hoteles 'Añoreta Suites', \n",
    "#'Casa Leon' y 'Arena Surf Hotel'. También reseteamos los índices. \n",
    "df = df.drop(df[df.Score.isna()].index)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "nans(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-trouble",
   "metadata": {},
   "source": [
    "Hemos hecho lo que hemos podido y ya sólo nos quedan dos columnas('Stars' y 'Beach') con muchos valores NaN. Los vamos a dejar como están, y en el caso de necesidad, si vamos a necesitar trabajar con estas variables, ya pensaremos lo que haremos con ellos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6bbd35",
   "metadata": {},
   "source": [
    "# FASE 2 EL ANÁLISIS.\n",
    "\n",
    "\n",
    "## El precio.\n",
    "\n",
    "En este caso práctico, somos los gerentes de un hotel de 3 estrellas ubicado en una zona de costa de la provincia de Alicante. Nuestro objetivo es analizar nuestros precios con respecto a la competencia directa.\n",
    "El objetivo de este análisis, es conocer si nuestros precios son competitivos y si es necesario modificarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa987045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Limpiamos la columna \"Beach\" que será usada para definir la competencia\n",
    "df['Beach'] = [str(x).replace(',', '.') for x in df['Beach']]\n",
    "df.loc[df.Beach=='nan', 'Beach'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561bb572",
   "metadata": {},
   "source": [
    "Para saber quién es nuestra competencia, nos vamos a centrar simplemente en aquellos hoteles de 3 estrellas, que tienen piscina como nosotros, y que están en la provincia de Alicante. Es posible modificar esta definición según la conveniencia de la temporada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb2c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la competencia: hoteles de 3 estrellas con piscina en la provincia de Alicante que aceptan mascotas.\n",
    "df_comp = df[(df[\"Stars\"] == 3.0) & (df[\"Postal_Code\"].str[0:2] == \"03\") & (df[\"Swimming pool\"] == 1)\n",
    "            & (df[\"Pets allowed\"] == 1)]\n",
    "df_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a452141",
   "metadata": {},
   "source": [
    "Definimos nuestros precios para los próximos 14 días"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6153d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df.Date.unique()\n",
    "# Próximos días como parámetro\n",
    "prox_dias = 14\n",
    "# Nuestros precios para los próximos días (aleatorios)\n",
    "our_rates = [random.randint(48,69) for j in range(0,prox_dias)]\n",
    "# Transformamos en dataframe\n",
    "our_df = pd.DataFrame(zip(dates, our_rates), columns=[\"Fecha\",\"Rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9746a",
   "metadata": {},
   "source": [
    "Comenzamos el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representación\n",
    "x_values1 = df_comp[\"Date\"].unique()\n",
    "y_values1 = df_comp.groupby([\"Date\"])[\"Price\"].mean()\n",
    "x_values2 = df[\"Date\"].unique()\n",
    "y_values2 = df.groupby([\"Date\"])[\"Price\"].mean()\n",
    "x_values3 = df[\"Date\"].unique()\n",
    "y_values3 = our_df[\"Rate\"]\n",
    "\n",
    "plt.figure()    #Figura. \n",
    "plt.bar(x_values1, y_values1, color = \"red\")          #El gráfico de la competencia\n",
    "plt.bar(x_values2, y_values2, color = \"blue\", fill = False)          #El gráfico global\n",
    "plt.plot(x_values3, y_values3, color = \"green\")          #El gráfico nuestro\n",
    "plt.title('Precios medios de la competencia los próximos días')      #El título\n",
    "ax = plt.subplot()                   #Axis\n",
    "ax.set_xticks(x_values3)             #Eje x\n",
    "ax.set_xticklabels(x_values3, rotation = 45)        #Etiquetas del eje x\n",
    "ax.set_xlabel('Fecha')  #Nombre del eje x\n",
    "ax.set_ylabel('Precio medio')  #Nombre del eje y\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una tabla report\n",
    "df_report = pd.DataFrame(zip(x_values1, our_rates, y_values1, y_values2, y_values1 - our_rates),\n",
    "            columns=[\"Fecha\", \"NuestroPrecio\", \"Media Competencia\", \"Media General\", \"Diferencia\"])\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos unas alertas que nos ayuden a rectificar los precios y adaptarlos a la competencia\n",
    "# Las alertas se activarán si nuestro precio está por debajo del 70% o por encima del 10% de la competencia.\n",
    "\n",
    "def alertas(df_report):\n",
    "    \"\"\"Genera a partir de un dataframe reporte, una serie de alertas\n",
    "        para corregir los precios\"\"\"\n",
    "    for index, row in df_report.iterrows():\n",
    "        if row[\"NuestroPrecio\"] < 0.7*row[\"Media Competencia\"]:\n",
    "            print(\"El día {} es necesario subir el precio\".format(row[\"Fecha\"]))\n",
    "        else:\n",
    "            if row[\"NuestroPrecio\"] > 1.1*row[\"Media Competencia\"]:\n",
    "                print(\"El día {} es necesario bajar el precio\".format(row[\"Fecha\"]))\n",
    "alertas(df_report)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-report",
   "metadata": {},
   "source": [
    "\n",
    "## La variable 'Comments' y su posible interpretación.\n",
    "\n",
    "La variable 'Comments' contiene el número de comentarios que cosechan los hoteles en el portal booking.com.\n",
    "Podemos interpretar el número de comentarios que recibe un hotel dado, como su nivel de popularidad entre los usuarios del portal. A mayor número de comentarios - mayor popularidad del hotel. \n",
    "\n",
    "Cada comentario lo podemos considerar como una opinión del cliente que haya adquirido su estancia en este hotel y, pues decide dejar un comentario. Además, realmente nos da igual si el comentario es negativo o positivo, ya que tanto si el cliente se quedó satisfecho como no, el cliente pagó su estancia. Esto implica que, de manera muy generalizada, podemos considerar que, por ejemplo, si un hotel dado recibe 1000 comentarios y su precio de estancia por una noche es de 80 euros, muy hipotéticamente podríamos considerar el siguiente flujo financiero: 1000 comentarios x 80 euros = 80000 euros. \n",
    "\n",
    "Con lo cual, a mayor número de comentarios -> mayor popularidad entre los clientes -> mayores flujos entrantes para el hotel. Desde el punto de vista del negocio, nos interesaría estudiar esta variable, su relación con otras variables, en general, nos interesa saber qué cualidades poseen los hoteles que cosechan mayores números de comentarios y gozan de mayor popularidad.\n",
    "\n",
    "En esta práctica, vamos a trabajar sólamente con las variables que tenemos en el dataset, más que nada por el tiempo que supone el scraping y por tener compasión y piedad con el portal booking.com, pero en teoría se podría recoger muchas más variables y hacer un análisis más completo y exhaustivo. Sin embargo, para demostrar la idea nos sobran las variables que ya tenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos el dataset de manera que sólo nos aparezcan hoteles 1 vez\n",
    "df_1 = df.drop_duplicates(subset = [\"Name\"])\n",
    "df_1.reset_index(drop=True, inplace=True)\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-transport",
   "metadata": {},
   "source": [
    "Para poder trabajar con la variable 'Comments', tenemos que pasar los valores al formato adecuado, ya que en el portal el número de comentarios contiene el separador de miles, lo que hace que, por ejemplo, 750 comentarios se interpretan como 750.00, lo que es correcto, mientras que los 1986 comentarios, que en el portal al contener el punto separador de miles(1.986) se interpretan tal cual, 1.986(1 comentario con 986 milésimas), lo que es incorrecto. Con lo cual, tenemos que pasar los valores mayores de 1000 comentarios al formato adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un vector con valores false/true, de manera que aquellos \n",
    "# valores que tienen parte decimal sean 'true', para luego simplemente\n",
    "# multiplicarlos por 1000, y los valores que no tienen parte \n",
    "# decimal(valores inferiores a 1000 comentarios en booking.com) sean 'false'\n",
    "vec=[]\n",
    "for i in range(0,len(df_1.Comments)):\n",
    "    vec.append(int(df_1.Comments.astype(str).str.split('.')[i][1])>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicamos valores con parte decimal por 1000 y dejamos los demás\n",
    "# como están\n",
    "df_1.loc[:,'Comments']=np.where(vec, df_1.Comments*1000, df_1.Comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-forth",
   "metadata": {},
   "source": [
    " Vemos que valores superiores a 1000 han tomado el formato adecuado, con lo cual ya podemos proceder con el análisis.\n",
    "\n",
    "Empezamos con simplemente ver las distribuciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_1['Comments'], color = 'blue', edgecolor = 'black', fill =True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hallamos el mínimo\n",
    "df_1.Comments.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hallamos el máximo\n",
    "df_1.Comments.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hallamos la media\n",
    "df_1.Comments.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-terrace",
   "metadata": {},
   "source": [
    "Vemos que hay más comentarios hasta 1000, mientras que el mínimo es sólo de 1 comentario y el máximo son 9268 comentarios. La media nos servirá de umbral, el número de comentarios inferior a la media los vamos a considerar como pocos(low), y los que superan a la media, los consideraremos como muchos(high). Vamos a crear una variable binaria, 'Comments_hl'(de Comments_high/low), que contendrá 1 para los comentarios que superan a la media(high) y 0 para comentarios inferiores a la media(low). Está variable la vamos a necesitar para generar las reglas de asociación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una variable binaria, 1 para los comentarios que superan a la\n",
    "# media, y 0 para los que no\n",
    "df_1['Comments_hl'] = np.where(df_1.Comments>df_1.Comments.mean(), 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.Comments_hl.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-evening",
   "metadata": {},
   "source": [
    "Representamos las frecuencias de la variable 'Stars'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos histograma de la variable 'Stars'\n",
    "plt.hist(df_1['Stars'], bins=np.arange(7)-0.5, color = 'blue', edgecolor = 'black')\n",
    "plt.xlabel('Stars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-roman",
   "metadata": {},
   "source": [
    "Vemos que hay más hoteles de 4 estrellas y muy pocos(que no llegan a los 100) de 1 y de 5 estrellas.\n",
    "\n",
    "Ahora sería interesante representar el número de estrellas en función de comentarios recebidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos las estrellas en función de comentarios\n",
    "df_1.pivot(columns='Comments_hl', values='Stars').plot.hist(bins=np.arange(7)-0.5, edgecolor='black')\n",
    "plt.xlabel('Stars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-praise",
   "metadata": {},
   "source": [
    "Está claro que los hoteles de 4 estrellas cosechan el mayor número de comentarios.\n",
    "\n",
    "La siguiente variable que vamos a ver es el precio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos las frecuencias de los precios\n",
    "plt.hist(df_1['Price'], color = 'blue', edgecolor = 'black')\n",
    "plt.xlabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-search",
   "metadata": {},
   "source": [
    "Vemos que prevalecen los precios hasta 100 euros. Vamos a aumentar un poco el gráfico exluyendo los precios inferiores a 300 euros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluímos los precios inferiores a 300 euros\n",
    "plt.hist(df_1.Price[df_1.Price>300], color = 'blue', edgecolor = 'black')\n",
    "plt.xlabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-frontier",
   "metadata": {},
   "source": [
    "Nos convendría discretizar la variable 'Price' partiéndola en intervalos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partimos en intervalos la variable Price\n",
    "df_1['Price_cat'] = pd.cut(df_1.Price, [1,100,200,300,400,500,600,700,800,900,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos los intervalos del precio\n",
    "df_1.Price_cat.value_counts().plot.bar(color = 'blue', edgecolor = 'black')\n",
    "plt.xlabel('Price intervals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-davis",
   "metadata": {},
   "source": [
    "Hay más precios hasta 100 euros. Vamos a representar los precios en función de los comentarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos los precios en función de los comentarios\n",
    "df_1.pivot(columns='Comments_hl', values='Price').plot.hist(edgecolor='black')\n",
    "plt.xlabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos los precios en función de comentarios excluyendo\n",
    "# los precios inferiores a 200 euros\n",
    "df_1[df_1.Price>200].pivot(columns='Comments_hl', values='Price').plot.hist(edgecolor='black')\n",
    "plt.xlabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos cuántos comentarios superiores a la media hay\n",
    "# en cado intervalos del precios\n",
    "df_1.Price_cat[df_1.Comments_hl==1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el valor relativo de los comentarios superiores a la\n",
    "# media en cada intervalo del precio\n",
    "for i,n in enumerate(df_1.Price_cat.value_counts()):\n",
    "    #print(n)\n",
    "    print(df_1.Price_cat[df_1.Comments_hl==1].value_counts().index[i],list(df_1.Price_cat[df_1.Comments_hl==1].value_counts())[i]/n)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-craps",
   "metadata": {},
   "source": [
    "Vemos que el intervalo (100, 200] contiene el mayor número de comentarios superiores a la media.\n",
    "\n",
    "Vamos a representar las variables 'Comments' y 'Price' una contra otra para ver si existe alguna relación de dependencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos los somentarios contra los precios\n",
    "plt.scatter(df_1.Comments, df_1.Price)\n",
    "plt.xlabel('Comments')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-offering",
   "metadata": {},
   "source": [
    "No se aprecia relación de dependencia clara entre el precio y el número de comentarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-sessions",
   "metadata": {},
   "source": [
    "## Reglas de asociación.\n",
    "\n",
    "\n",
    "También podemos aplicar el algoritmo de las reglas de asociación al conjunto de atributos, para descubrir algunos patrones que ocurren en común y que tienen alguna consecuencia. En este caso, nos interesaría descubrir algunas causas(reglas) que hacen que el hotel goce de popularidad(un número de comentarios alto), nos interesa descubrir qué conjunto de atributos(por ejemplo:['Airport Shuttle (free)','BBQ facilities', 'Free WiFi Internet Access Included', '4_Stars']) tienen los hoteles que están altamente populares(tienen un número de comentarios alto) en el portal booking.com. \n",
    "\n",
    "Este conjunto de atributos, ['Airport Shuttle (free)','BBQ facilities', 'Free WiFi Internet Access Included', '4_Stars'], se denomina 'antecedente'(X), y el número alto de comentarios(Y), es el consecuente. Con lo cual, la definición formal de una regla sería la implicación X=>Y, que siguiendo con el ejemplo de nuestro caso sería: ['Airport Shuttle (free)','BBQ facilities', 'Free WiFi Internet Access Included', '4_Stars'] => alto nº de comentarios. O sea, nos interesa descubrir conjuntos de antecedentes que implican alto nº de comentarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para poder aplicar el algoritmo de reglas de asociación, tenemos que pasarle\n",
    "# el dataset en forma binaria(1 o 0). Con lo cual, añadimos a los atributos\n",
    "# binarios que ya teníamos, algunos más.\n",
    "df_1['nan'] = np.where(df_1.Stars=='nan', 1,0)\n",
    "df_1['1_Star'] = np.where(df_1.Stars==1, 1,0)\n",
    "df_1['2_Stars'] = np.where(df_1.Stars==2, 1,0)\n",
    "df_1['3_Stars'] = np.where(df_1.Stars==3, 1,0)\n",
    "df_1['4_Stars'] = np.where(df_1.Stars==4, 1,0)\n",
    "df_1['5_Stars'] = np.where(df_1.Stars==5, 1,0)\n",
    "df_1['Price_high'] = np.where(df_1.Price>df_1.Price.mean(), 1,0)\n",
    "df_1['Price_low'] = np.where(df_1.Price<df_1.Price.mean(), 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El proceso de generación de las reglas de asociación consiste de\n",
    "# 2 etapas: primero se genera un dataset de artículos frecuentes,\n",
    "# 'frequent_itemsets', el umbral de frecuencia 'min_support' lo\n",
    "# establecemos nosotros, lo que quiere decir que quremos generar\n",
    "# un subset, cuyos elementos(subconjuntos de atributos) ocurren\n",
    "# simultáneamente con una frecuencia mínima que establezcamos,\n",
    "# por ejemplo, en un 10% de registros(transacciones), y luego\n",
    "# pasamos el 'frequent_itemsets' al algoritmo 'association_rules'\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "\n",
    "# Generamos el dataset de artículos frecuentes\n",
    "frequent_itemsets = apriori(df_1.loc[:, ['Airport Shuttle (free)',\n",
    "       'BBQ facilities', 'Free WiFi Internet Access Included', 'Wireless Lan',\n",
    "       'Daily maid service', 'Swimming pool', 'Pets allowed', 'Free Parking',\n",
    "       'Spa & Wellness Centre', 'Fitness Room', 'Private Beach Area',\n",
    "       'Heating', 'Family Rooms', 'Coffee/Tea maker', 'Restaurant',\n",
    "       'Beach front', 'Garden', 'Bar', 'Terrace',\n",
    "       'Rooms/Facilities for Disabled', 'Elevator', 'Non Smoking Rooms',\n",
    "       '24 hour Front Desk', 'Room-service', 'Airport Shuttle',\n",
    "       'Parking (fee required)', 'nan', '1_Star', '2_Stars', '3_Stars', \n",
    "        '4_Stars', '5_Stars','Price_high','Price_low','Comments_hl']], min_support=0.1, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos las reglas de asociación, usando el soporte('support') como métrica\n",
    "# con un umbral mínimo del 10%. Lo que quiere decir que queremos generar las\n",
    "# reglas que aparecen como mínimo en un 10% de transacciones.\n",
    "rules = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0.1)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, filtramos las reglas obtenidas de manera que sólo queremos\n",
    "# aquellas reglas, cuyo consecuente es 'Comments_hl'(el nº de comentarios alto)\n",
    "# y cuya confianza es mayor del 50%\n",
    "lft = rules[(rules.consequents=={'Comments_hl'})&\n",
    "            (rules['confidence']>0.5)].sort_values('confidence', ascending=False)\n",
    "lft.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las 7 primeras reglas con su correspondiente soporte,\n",
    "# confianza y lift\n",
    "for index, row in lft.iloc[0:7,[0,4,5,6]].iterrows():\n",
    "    print(str(row.antecedents)+' support:'+str(round(row.support,2))+\n",
    "          '; confidence:'+str(round(row.confidence,2))+'; lift: '+str(round(row.lift,2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-award",
   "metadata": {},
   "source": [
    "La interpretación de las reglas de asociación generadas podría ser la siguiente.\n",
    "\n",
    "La primera regla nos dice que en el dataset hay un 10% de transacciones(registros), support:0.1, con los atributos ['Swimming pool'(piscina), 'Parking(fee required)' y Price_low(precio por debajo de la media)] como antecedente, y de estos 10%, hay un 69% que tienen 'Comments_hl'(el nº alto de comentarios) como consecuente. Es decir, por ejemplo de los 100 transacciones(registros), los 10 tienen este conjunto de atributos, y de estos 10, los casi 7 implican un nº de comentarios alto. Con lo cual, podríamos decir que si un hotel posee estos atributos, con una probabilidad de casi 70%, este hotel tendrá un nº de comentarios elevado.\n",
    "\n",
    "El indicador 'lift' nos indica la tasa de soporte observada sobre la esperada si X e Y fueran independientes. Un lift < 1 quiere decir que la tasa de soporte observada aparece menos veces que la esperada, mientras que un lift > 1 nos dice que la tasa de soporte observada aparece más veces de la esperada. En nuestro caso, lift(X=>Y)=1.94 quiere decir que es más probable que los sucesos X e Y ocurran juntos, que por separado. En otras palabras, un lift > 1 nos dice que es más probable que el conjunto de atributos ['Swimming pool'(piscina), 'Parking(fee required)' y Price_low(precio por debajo de la media)] implique un nº de comentarios alto, que no. Con lo cual, un lift=1.94 indica que esta regla es bastante fuerte.\n",
    "\n",
    "La segunda regla tiene los siguientes atributos como antecedente:['Swimming pool', '4_Stars', 'Bar', 'Parking (fee required)']. Este conjunto aparece en un 11% de transacciones(registros), support=0.11. De este 11% de transacciones, en un 69% implica el nº de comentarios alto, confidence=0.69. Un lift=1.93 nos indica que esta regla también es bastante fuerte. Con lo cual, podríamos afirmar que si un hotel tiene piscina, bar, parking y es de 4 estrellas, con una probabilidad de casi 70% este hotel cosecharía un nº de comentarios alto.\n",
    "\n",
    "Todas las demás reglas se interpretan de la misma manera, sólo que cambian los conjuntos de atributos, y los indicadores 'support', 'confidence' y 'lift' varían ligeramente.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
